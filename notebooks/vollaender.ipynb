{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easyocr\n",
      "  Downloading easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from easyocr) (2.2.2)\n",
      "Collecting torchvision>=0.5 (from easyocr)\n",
      "  Downloading torchvision-0.17.2-cp310-cp310-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting opencv-python-headless (from easyocr)\n",
      "  Downloading opencv_python_headless-4.9.0.80-cp37-abi3-macosx_10_16_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scipy in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from easyocr) (1.13.1)\n",
      "Requirement already satisfied: numpy in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from easyocr) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from easyocr) (10.3.0)\n",
      "Collecting scikit-image (from easyocr)\n",
      "  Downloading scikit_image-0.23.2-cp310-cp310-macosx_10_9_x86_64.whl.metadata (14 kB)\n",
      "Collecting python-bidi (from easyocr)\n",
      "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: PyYAML in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from easyocr) (6.0.1)\n",
      "Collecting Shapely (from easyocr)\n",
      "  Using cached shapely-2.0.4-cp310-cp310-macosx_10_9_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting pyclipper (from easyocr)\n",
      "  Downloading pyclipper-1.3.0.post5-cp310-cp310-macosx_10_9_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting ninja (from easyocr)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: filelock in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from torch->easyocr) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from torch->easyocr) (4.12.0)\n",
      "Requirement already satisfied: sympy in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from torch->easyocr) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from torch->easyocr) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from torch->easyocr) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from torch->easyocr) (2024.5.0)\n",
      "Requirement already satisfied: six in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from python-bidi->easyocr) (1.16.0)\n",
      "Collecting imageio>=2.33 (from scikit-image->easyocr)\n",
      "  Downloading imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->easyocr)\n",
      "  Downloading tifffile-2024.5.22-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: packaging>=21 in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from scikit-image->easyocr) (24.0)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->easyocr)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from jinja2->torch->easyocr) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/terezka/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages (from sympy->torch->easyocr) (1.3.0)\n",
      "Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.17.2-cp310-cp310-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl (270 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.6/270.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.9.0.80-cp37-abi3-macosx_10_16_x86_64.whl (55.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-macosx_10_9_x86_64.whl (145 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.7/145.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
      "Downloading scikit_image-0.23.2-cp310-cp310-macosx_10_9_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached shapely-2.0.4-cp310-cp310-macosx_10_9_x86_64.whl (1.4 MB)\n",
      "Downloading imageio-2.34.1-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading tifffile-2024.5.22-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.5/225.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyclipper, ninja, tifffile, Shapely, python-bidi, opencv-python-headless, lazy-loader, imageio, scikit-image, torchvision, easyocr\n",
      "Successfully installed Shapely-2.0.4 easyocr-1.7.1 imageio-2.34.1 lazy-loader-0.4 ninja-1.11.1.1 opencv-python-headless-4.9.0.80 pyclipper-1.3.0.post5 python-bidi-0.4.2 scikit-image-0.23.2 tifffile-2024.5.22 torchvision-0.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This part scans an image of letter and applies an OCR text recognition in order\n",
    "### to get the text of the image and transforms it to txt and stores it\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "\n",
    "def process_images(image_directory, output_directory):\n",
    "    \"\"\"\n",
    "    Process all images in the specified directory using OCR and save the extracted text to text files.\n",
    "\n",
    "    Args:\n",
    "        image_directory (str): Path to the directory containing the image files.\n",
    "        output_directory (str): Path to the directory where the output text files will be saved.\n",
    "    \"\"\"\n",
    "    # initialize the OCR reader\n",
    "    reader = easyocr.Reader(['de'])\n",
    "\n",
    "    # create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # iterate through all files in the directory\n",
    "    for filename in os.listdir(image_directory):\n",
    "        # check if file is an image\n",
    "        if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png') or filename.endswith('.pdf'):\n",
    "            # make the full path to the image file\n",
    "            image_path = os.path.join(image_directory, filename)\n",
    "\n",
    "            # open image for OCR\n",
    "            try:\n",
    "                with open(image_path, 'rb') as image_file:\n",
    "                    image_bytes = image_file.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # extract text from image using easyocr\n",
    "            try:\n",
    "                result = reader.readtext(image_bytes)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # extracted text\n",
    "            text = '\\n'.join([entry[1] for entry in result])\n",
    "\n",
    "            # define output path to save extracted text\n",
    "            output_path = os.path.join(output_directory, f'{os.path.splitext(filename)[0]}.txt')\n",
    "\n",
    "            # check if output file already exists\n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"Skipping {filename}. Output file {output_path} already exists.\")\n",
    "                continue\n",
    "\n",
    "            # save extracted text to a text file\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(text)\n",
    "\n",
    "            # print a confirmation message about the text extraction\n",
    "            print(f'Text from {filename} has been saved in: {output_path}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/Users/terezka/code/katia-si/better-letter-API/raw_data/1000014343.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/terezka/code/katia-si/better-letter-API/raw_data/german_ocr_text\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# process images in the specified directory\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m, in \u001b[0;36mprocess_images\u001b[0;34m(image_directory, output_directory)\u001b[0m\n\u001b[1;32m     21\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_directory)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# iterate through all files in the directory\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_directory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# check if file is an image\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# make the full path to the image file\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_directory, filename)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/Users/terezka/code/katia-si/better-letter-API/raw_data/1000014343.jpg'"
     ]
    }
   ],
   "source": [
    "# define path to the directory containing the image files\n",
    "image_directory = \"/Users/terezka/code/katia-si/better-letter-API/raw_data/1000014343.jpg\" \n",
    "\n",
    "# define the path to the directory where the output text files will be saved\n",
    "output_directory = '/Users/terezka/code/katia-si/better-letter-API/raw_data/german_ocr_text'\n",
    "\n",
    "# process images in the specified directory\n",
    "process_images(image_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/1000014343_cldn.txt\n",
      "Cleaned text has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/1000014342_cldn.txt\n",
      "Cleaned text has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/letter_school_hamburg_cldn.txt\n",
      "Cleaned text has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/Ist-oder-soll_cldn.txt\n",
      "Cleaned text has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/letter_arbeitsamt_internet_cldn.txt\n"
     ]
    }
   ],
   "source": [
    "### this part cleans the text data from the letters from intendation or hyphenation and so on\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    # Handle hyphenated line breaks\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    # Replace remaining newlines with spaces\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_and_save_files(input_directory, output_directory):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Iterate through all files in the input directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        # Check if the file is a text file\n",
    "        if filename.endswith('.txt'):\n",
    "            # Construct the full path to the input file\n",
    "            input_file_path = os.path.join(input_directory, filename)\n",
    "            # Construct the full path to the output file\n",
    "            output_file_path = os.path.join(output_directory, filename.replace('.txt', '_cldn.txt'))\n",
    "\n",
    "            # Check if the output file already exists\n",
    "            if os.path.exists(output_file_path):\n",
    "                print(f'Skipping {filename}. Output file {output_file_path} already exists.')\n",
    "                continue\n",
    "\n",
    "            # Read the content of the input file\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                extracted_text = file.read()\n",
    "\n",
    "            # Clean the extracted text\n",
    "            cleaned_text = clean_extracted_text(extracted_text)\n",
    "\n",
    "            # Write the cleaned text to the output file\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(cleaned_text)\n",
    "\n",
    "            print(f'Cleaned text has been saved to: {output_file_path}')\n",
    "\n",
    "# Define the input directory containing the extracted text files\n",
    "input_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text'\n",
    "\n",
    "# Define the output directory where cleaned text files will be saved\n",
    "output_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned'\n",
    "\n",
    "# Clean and save text files in the input directory\n",
    "clean_and_save_files(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated text has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_translation/letter_arbeitsamt_internet_cldn_en.txt\n",
      "translated text has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_translation/1000014342_cldn_en.txt\n",
      "translated text has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_translation/1000014343_cldn_en.txt\n",
      "translated text has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_translation/Ist-oder-soll_cldn_en.txt\n",
      "translated text has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_translation/letter_school_hamburg_cldn_en.txt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary/letter_school_hamburg_cldn_en_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary/letter_arbeitsamt_internet_cldn_en_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary/1000014343_cldn_en_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary/Ist-oder-soll_cldn_en_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary/1000014342_cldn_en_sum.txt\n"
     ]
    }
   ],
   "source": [
    "### summarize german text short\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import os\n",
    "\n",
    "# load pre-trained model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# function to generate summary\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, min_length=30, max_length=200, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# define input and output directories\n",
    "input_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/english_translation'\n",
    "output_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary'\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# generate summary for each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    # check if file is a text file\n",
    "    if filename.endswith('.txt'):\n",
    "        # construct the full path to the input file\n",
    "        input_file_path = os.path.join(input_directory, filename)\n",
    "\n",
    "        # construct the full path to the output file\n",
    "        output_file_path = os.path.join(output_directory, f'{os.path.splitext(filename)[0]}_sum.txt')\n",
    "\n",
    "        # check if the output file already exists\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f'skipping {filename}. output file {output_file_path} already exists.')\n",
    "            continue\n",
    "\n",
    "        # read the content of the input file\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # generate summary\n",
    "        summary = generate_summary(text)\n",
    "\n",
    "        # write summary to the output file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(summary)\n",
    "\n",
    "        print(f'summary has been generated and saved to: {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/letter_arbeitsamt_internet_cldn_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/1000014342_cldn_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/1000014343_cldn_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/Ist-oder-soll_cldn_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/letter_school_hamburg_cldn_sum.txt\n"
     ]
    }
   ],
   "source": [
    "### trying a longer summarization of the german text\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import os\n",
    "\n",
    "# load pre-trained BART model and tokenizer for German\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Shahm/bart-german\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Shahm/bart-german\")\n",
    "\n",
    "# function to generate summary with adjusted parameters\n",
    "def generate_summary_longer(text: str) -> str:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=400,  # increase max_length to make the summary longer\n",
    "        min_length=150,  # adjust min_length accordingly\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# define input and output directories\n",
    "input_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/'\n",
    "output_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/'\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# generate summary for each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    # check if file is a text file\n",
    "    if filename.endswith('.txt'):\n",
    "        # construct the full path to the input file\n",
    "        input_file_path = os.path.join(input_directory, filename)\n",
    "\n",
    "        # construct the full path to the output file\n",
    "        output_file_path = os.path.join(output_directory, f'{os.path.splitext(filename)[0]}_sum.txt')\n",
    "\n",
    "        # check if the output file already exists\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f'skipping {filename}. output file {output_file_path} already exists.')\n",
    "            continue\n",
    "\n",
    "        # read the content of the input file\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # generate summary\n",
    "        summary = generate_summary_longer(text)\n",
    "\n",
    "        # write summary to the output file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(summary)\n",
    "\n",
    "        print(f'summary has been generated and saved to: {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/1000014343_cldn_sum.txt\n",
      "Translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/1000014342_cldn_sum.txt\n",
      "Translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/letter_school_hamburg_cldn_sum.txt\n",
      "Translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/letter_arbeitsamt_internet_cldn_sum.txt\n",
      "Translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/Ist-oder-soll_cldn_sum.txt\n"
     ]
    }
   ],
   "source": [
    "### translate german summarized tex to english\n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import os\n",
    "\n",
    "# load the MarianMTModel and tokenizer for translation\n",
    "tokenizer_translate = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
    "model_translate = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
    "\n",
    "# function to translate German text to English\n",
    "def translate_to_english(german_text):\n",
    "    # tokenize the German text\n",
    "    inputs = tokenizer_translate(german_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # generate English translation\n",
    "    translated = model_translate.generate(**inputs)\n",
    "    # decode the translated text\n",
    "    translated_text = tokenizer_translate.batch_decode(translated, skip_special_tokens=True)\n",
    "    return translated_text[0]\n",
    "\n",
    "# define input and output directories for German summaries\n",
    "input_directory_german = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/'\n",
    "output_directory_english = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/'\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory_english):\n",
    "    os.makedirs(output_directory_english)\n",
    "\n",
    "# translate and save summaries for each file in the input directory\n",
    "for filename in os.listdir(input_directory_german):\n",
    "    # check if file is a text file\n",
    "    if filename.endswith('.txt'):\n",
    "        # read the German summary\n",
    "        with open(os.path.join(input_directory_german, filename), 'r', encoding='utf-8') as file:\n",
    "            german_summary = file.read()\n",
    "\n",
    "        # translate German summary to English\n",
    "        english_summary = translate_to_english(german_summary)\n",
    "\n",
    "        # construct the full path to the output file\n",
    "        output_file_path_english = os.path.join(output_directory_english, filename)\n",
    "\n",
    "        # write the translated summary to the output file\n",
    "        with open(output_file_path_english, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(english_summary)\n",
    "\n",
    "        print(f'translated summary has been saved to: {output_file_path_english}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Differenz zwischen dem von den Eltern zu zahlenden Maximalpreis von 4,35 Euro und der neuen Preisobergrenze für die Caterer übernimmt die Schulbehörde. Das gilt für alle Mittagessen; auch die der vollzahlendsten Schülerinnen und Schülern. Damit setzt der Hamburger Senat konsequent die seit 2020 begonnene Linie durch. Mit freundlichen Grüßen Ihr Ganzirksame Ganztagsreferat pro state of the sun: Die Freie und Hansestadt Hamburg.\n"
     ]
    }
   ],
   "source": [
    "### playground to test several paramters in order to get the best translation\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load BART model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Shahm/bart-german\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Shahm/bart-german\")\n",
    "\n",
    "# Define your original text\n",
    "original_text = \"\"\"\n",
    "Schulisches Mittagessen: Neuer Maximalpreis für das Schuljahr 2023/24 und neue Preisobergrenze für Caterer ab dem 01.08.2023 Sehr geehrte Eltern, mit diesem Schreiben möchten wir Sie darüber informieren; dass auf Basis der allgemeinen Preisentwicklung der von den Eltern zu zahlende Höchstpreis für das Mittagessen zum 1. August 2023 um 20 Cent auf 4,35 Euro angehoben wird. Angesichts der nach wie vor hohen Lebensmittelpreise deckt dieser Beitrag derzeit nicht die tatsächlichen Kosten für ein Mittagessen: Die Caterer können daher ab dem 01.08.2023 bis zu 4,80 Euro Mittagessen abrechnen Die Differenz zwischen dem von den Eltern zu zahlenden Maximalpreis von 4,35 Euro und der neuen Preisobergrenze für die Caterer übernimmt die Schulbehörde und rechnet die Differenz direkt mit den Caterern ab. Das gilt für alle Mittagessen; auch die der vollzahlenden Schülerinnen und Schüler. Damit setzt der Hamburger Senat konsequent die seit 2020 begonnene Linie durch; im Sinne der Familien; Kinder und Jugendlichen ein schulisches Mittagessen zu vertretbaren Preisen zu sichern und den an den Hamburger Schulen tätigen Cateringunternehmen angemessene Preise zu ermöglichen: Insgesamt übernehmen die Freie und Hansestadt Hamburg und der Bund deutlich mehr als 50 Prozent der Kosten aller schulischen Mittagessen, um für alle Schülerinnen und Schüler an Hamburgs Schulen ein gesundes Mittagessen zu gewährleisten: Mehr Informationen hierzu finden Sie bei Bedarf unter Mittaqessen_für_die_Hamburqer_Schulen hamburq de. Mit freundlichen Grüßen Ihr Ganztagsreferat pro\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary with longer max_length\n",
    "def generate_summary_longer(text: str) -> str:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=400,  # Increase max_length to make the summary longer\n",
    "        min_length=150,  # Adjust min_length accordingly\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Generate longer summary\n",
    "summary_longer = generate_summary_longer(original_text)\n",
    "\n",
    "# Print the longer summary\n",
    "print(summary_longer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
